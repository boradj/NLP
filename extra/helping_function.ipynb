{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3AOJ1W7TJrqTohBDotCUs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boradj/NLP/blob/main/extra/helping_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
      ],
      "metadata": {
        "id": "sxbq0NfZVuXl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_results(y_true, y_pred):\n",
        "  '''\n",
        "  function is to calcualate model accuracy, precision, recall and f1 score for binary classification model.\n",
        "  \n",
        "  Args:\n",
        "  ---\n",
        "  y_true :- true label in the form of a 1D array\n",
        "  y_pred :- predicted labels(1D array)\n",
        "\n",
        "  '''\n",
        "  #model accuracy\n",
        "  model_accuracy = accuracy_score(y_true, y_pred)*100\n",
        "\n",
        "  # using weighted average calcualting model precision, recall and f1 score.\n",
        "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "\n",
        "  model_results = {\"accuracy\" :- model_accuracy,\n",
        "                   \"precision\" :- model_precision,\n",
        "                   \"recall\" :- model_recall,\n",
        "                   \"f1\" :- model_f1}\n",
        "  return model_results\n",
        "\n"
      ],
      "metadata": {
        "id": "-s5IVol1WO99"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comparing two different model result\n",
        "def compare_result(model_1_result, model_2_result):\n",
        "  for key, values in model_1_result.items:\n",
        "    print(f\"model_1{key} : {values:.2f}, model_2{key} : {model_2_result[key]:.2f}, Difference : {model_2_result[key] - values:.2f} \")"
      ],
      "metadata": {
        "id": "dFDU6D0HmXCR"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}